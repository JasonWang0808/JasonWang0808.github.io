<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[B-Tree(AVL, Red-Black, B(2-3))]]></title>
    <url>%2F2019%2F03%2F01%2FB-tree%2F</url>
    <content type="text"><![CDATA[Red-Black Tree Balance factor显示了是否要进行旋转。BF = height_left - height_right 性质1. 根结点必须是黑色2. 红色结点的子结点必须是黑色3. 根结点到任意一个叶子结点路径上的黑色结点数相同How to fix 当违反了规则的时候，我们把它称为R.P.V(Red Properties Violation) 1. Recoloring — when the parent’s sibling is red, but you need recolor the root to be black in the end;2. Reconstruting — when the parent’s sibling is black or nullHints:1. 每一次插入的都是红色2. left-rotate 是指 Grandparent 变为parent的左孩子123G.right = P.right;P.right = G;return P; // 返回新的根结点 3. 复杂度全部为log(N),遍历还是n。B-TreeAVL解决了BST插入不平衡的问题，但是要进行旋转，复杂度较高，因此提出2-3Tree，使得插入更加简便 拥有一个内部结点key以及两个两个子结点，这个叫做2结点; 拥有两个内部结点key以及三个子结点，这个叫做3结点 这里可以看到一个对B-Tree简单的笔记，介绍了性质 可以参考这里2-3Tree的插入过程 B+ tree B+ tree是Btree的升级，主要改变在以下几个方面 B+ tree独有的性质1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。B+ Tree的好处1. 单一结点可以储存更多的元素，减少IO的次数2. 所有的data（Satellite Information）都存在叶结点，每一次的查询都很稳定3. 叶结点中有指针相互连接，使得查询某个范围中的数据集合更加方便（对于B树可能要不断回溯才能找到下一范围）]]></content>
  </entry>
  <entry>
    <title><![CDATA[Leetcode classical problem]]></title>
    <url>%2F2019%2F03%2F01%2Fleetocde_classic%2F</url>
    <content type="text"><![CDATA[big number multiply will over flow if you transfer them into number and do the product. 因此逐位相乘，下面这行代码可以说明一切12vector1[i+j] = (string1[i] - '0' * string2[i] - '0');// 利用了乘法的性质，把相同位数的加到一起// vector1中会存在有些元素比10大，因此从尾到头遍历一遍，进行进位。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Leetocde回溯法(Back Tracking)]]></title>
    <url>%2F2019%2F02%2F28%2F%E5%9B%9E%E6%BA%AF%2F</url>
    <content type="text"><![CDATA[出错原因对于这种组合问题，只有三种情况1. （多次）选择当前2. 选择当前3. 不选当前很多时候用递归，可能两个分支会递归到同一种状态，最后导致结果的重复（见下图）因此我们可以将函数的主体改为一个for循环12345for(i : I)&#123; vector1.push_back(i); // 选择当前 Solve(); // 假定选择当前进行处理 vector.pop_back(); // 不选当前&#125; leetcode 39, 40 (microsoft) the goal is to find 39可以利用上面的方法进行求解。40 又多了一个trick [1,1,2,5,1] , 1+2+5 = 2+5+1 = 8,因此会得到相同的3组解。 我们把这个理解成，“只看个数”不看顺序，假如3个1中，出现了一个没选，那剩下的都不能选。 比如牛肉面加肉，有三块牛腩可以加。但是第一块很小，第二块中等，第三块最大。每一块价格相同，不能跳着选。要想吃到第三块牛肉就要把第一块第二块都选中。 中途放弃的后面的也别想要。这就是这个题如何避免重复解的原理]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Structure(4) The using of Set/map]]></title>
    <url>%2F2019%2F01%2F08%2FDS_SetMap%2F</url>
    <content type="text"><![CDATA[1. Set的使用 可用于储存在计算过程中的过程，leetcode202 happy number就需要储存计算中曾经出现过哪些数字。 使用insert排着插入即可 Leetcode 349求补集注意vector和set之间可以直接一键转换。123#include &lt;set&gt;set&lt;int&gt; s1;vector&lt;int&gt; vector1(s1.begin(), s1.end()); 2. map类似于python中的dict3. 补充map和set在底层都是一棵平衡二叉树，有unordered_map和unordered_set底层是哈希表实现]]></content>
      <categories>
        <category>DS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Structure(1) Linked List]]></title>
    <url>%2F2019%2F01%2F07%2FDT_LinkedList1%2F</url>
    <content type="text"><![CDATA[Reverse Linked List Reverse Linked List 2考察链表的指针操作 Notes: 要充分注意首尾是否指针为空 Tips：虚拟头节点 可以定义一个dummyHead作为head前面的标志性指针, 好处是，不管head怎样变化，只要调用其next总能找到现在的head（203，82） 12ListNode *dummyHead = ListNode(-1);dummyHead-&gt;next = head; 对于特定的92号问题，反转链表，我们可以通过一个函数返回两个数据： 最左边的元素 原本最左边的元素的next 在链表中穿针引线（23，24，124，125， 237）双指针（61，143，234，19）]]></content>
      <categories>
        <category>DT</category>
      </categories>
      <tags>
        <tag>DT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Structure(1) Linked List]]></title>
    <url>%2F2019%2F01%2F07%2FDS_LinkedList%2F</url>
    <content type="text"><![CDATA[Reverse Linked List Reverse Linked List 2考察链表的指针操作 Notes: 要充分注意首尾是否指针为空 Tips：虚拟头节点 可以定义一个dummyHead作为head前面的标志性指针, 好处是，不管head怎样变化，只要调用其next总能找到现在的head（203，82） 12ListNode *dummyHead = ListNode(-1);dummyHead-&gt;next = head; 对于特定的92号问题，反转链表，我们可以通过一个函数返回两个数据： 最左边的元素 原本最左边的元素的next 在链表中穿针引线（23，24，124，125， 237）双指针（61，143，234，19）]]></content>
      <categories>
        <category>DS</category>
      </categories>
      <tags>
        <tag>DS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Structure(2) Stack VS queue]]></title>
    <url>%2F2019%2F01%2F07%2FDS_stack%2F</url>
    <content type="text"><![CDATA[一、基本介绍Stack，又名堆，后进先出问题。非常经典的一个括号匹配问题就是出自Stack之手。Queue，又名队列，先进先出，常用于对“TreeNode”的层级问题的解答，因为每一层都可以看作是一个队列来进行处理。常用Vector&lt;Vector&gt;来进行计算 102,103,104,199都可以用队列来解答 123456// Stack常用用法#include &lt;stack&gt;stack&lt; pair &lt;Node*, int&gt;&gt; s1; //设立一个包含pair的Stack s1.top();s1.push_back();s1.pop(); 123456// queue常用用法#include &lt;queue&gt;queue&lt;&lt; pair &lt;Node*, int&gt;&gt; q1; //设立一个包含pair的Stack s1.top();s1.push();s1.pop(); 二、图论应用 类似练习127，126例如leetcode279题，寻找相加平方数最少的一种方案。我们可以将其想像成求无权边的最短路径，使用BFS 例如计算9，首先8–&gt;9(1), 5–&gt;9(2), 0–&gt;9(3) , 9就连接了三个顶点分别是0,5,8 1234567891011121314151617181920212223242526272829303132//Leetcode279class Solution &#123;public: int numSquares(int n) &#123; vector&lt;bool&gt; isvisit(n+1, 0); isvisit[n] = 1; queue&lt; pair&lt;int, int&gt;&gt; queue1; queue1.push(make_pair(n, 0)); while(queue1.empty() == 0)&#123; int Node_n = queue1.front().first; int dis = queue1.front().second; queue1.pop(); for(int i = 1; ; i ++)&#123; int temp = i * i; if(temp &gt; Node_n) break; else if(temp == Node_n)&#123; return dis + 1; &#125; else&#123; if(isvisit[Node_n - temp] == 0 )&#123; queue1.push(make_pair(Node_n - temp, dis+1)); isvisit[Node_n - temp] = 1; &#125; &#125; &#125; &#125; &#125;&#125;; 三、优先队列优先队列其实就是一个堆，只是能应用的逻辑相对比较简单 123456# include&lt;queue&gt;priority_queue&lt;int, vector&lt;int&gt;, function&lt;bool(int, int)&gt; &gt; pq1(mycmp);bool mycpmp(int a, int b)&#123; return a &lt; b; //大的元素在队列首&#125; 例题：Leetcode347。 Tips： 建立一个map，类似Wordcount 队列的pair运用第一个元素比较，因此要把pair的第一个元素设置成频率。 while循环别把empty的标志设置反 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: vector&lt;int&gt; topKFrequent(vector&lt;int&gt;&amp; nums, int k) &#123; assert(k &gt; 0); unordered_map&lt;int ,int&gt; freq; for( int i = 0 ; i &lt; nums.size() ; i ++)&#123; freq[nums[i]] ++; &#125; assert(k &lt;= freq.size()); priority_queue&lt; pair&lt;int, int&gt;, vector&lt;pair&lt;int, int&gt;&gt;, greater&lt;pair&lt;int,int&gt;&gt; &gt; pq; for( unordered_map&lt;int, int&gt;::iterator iter = freq.begin() ; iter != freq.end() ; iter ++)&#123; if(pq.size() == k)&#123; if(iter-&gt;second &gt; pq.top().first)&#123; pq.pop(); pq.push(make_pair(iter-&gt;second, iter-&gt;first)); &#125; &#125; else&#123; pq.push(make_pair(iter-&gt;second, iter-&gt;first)); &#125; &#125; vector&lt;int&gt; res; while(!pq.empty())&#123; res.push_back(pq.top().second); pq.pop(); &#125; return res; &#125;&#125;;]]></content>
      <categories>
        <category>DS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[收到offer之后的申请流程]]></title>
    <url>%2F2018%2F11%2F27%2Fwisc_prepare%2F</url>
    <content type="text"><![CDATA[我们这里以UW-Madison为例讲一下留学得到offer后的操作先给出一个流程图，大概说一下整个的流程 1. 首先要做的当然是Accept offer2. 接着我们需要拿到一个叫做I-20的文件（这个每个学校的操作可能各有不同） Checklist 4. 中列出了需要的文件 申请I20在UW-Madison一共需要4个文件 Financial Verification Form 存款证明 Student Visa Information and Mailing Form Copies of passport pages showing your identity and full legal name 准备好了四个文件，将pdf发给onwisconsin@admissions.wisc.edu就结束了I20的申请3. 等待I-20，这是一个漫长的过程，大概需要两个半星期4.1 拿到I-20之后就要申请F-1签证了 这里UW-Madison给出了一些F-1签证的tips 但是个人感觉上述说的不是很具体，这里给出两个比较生动形象的公众号贴文 (文章1，文章2)看完这两个文章，并准备好相应的材料就可以预约面签了 4.2 与此同时我们应该开始准备Placement Test了 这里是FAQ 届时会有邮件通知我们需要参加什么类别的考试点击-&gt; 进入注册placement test 同样可以在SOAR 2.中看到详细信息 5. 拿到F-1签证准备好行李顺顺利利去上学]]></content>
  </entry>
  <entry>
    <title><![CDATA[how to use Ubuntu and Git to push projects]]></title>
    <url>%2F2018%2F11%2F19%2FUbuntu-git%2F</url>
    <content type="text"><![CDATA[Step1. 配置SSH key并输入到github上 见下面 Step2. 手动添加仓库或者直接cloneStep3. 添加成为contributor首先下载git，这个不用我说了在github上建立一个名字为XXX的仓库作为你要托管的对象，这里我采用learn-repo作为我的名字 JasonWang0808/Learn-repo 在Ubuntu下生成SSH key来连接自己的github，可以参考这个博文, 此处也不啰嗦 接下来在ubuntu中建立一个文件夹作为本地仓库 12345678910111213141516# 初始化git init# 建立文件（以及做出类似改动）touch README.mdgit add README.md# 提交本次的改动，注意后面的注释参数不要拉下git commit -m 'first_commit'# 建立远程连接 origin就是仓库名字而已，允许修改git remote add origin https://github.com/JasonWang0808/learn-repo.git# 将新的仓库内容push到刚刚命名的origin，并且为master分支 git push origin master--------------------- 遇到的坑 直接复制文件夹过来上传后是灰色的 不知道为啥，自己新建然后cp吧 报错：error:failed to push som refs to…… git pull –rebase origin master 可能是本地和远程有区别等原因]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hidden Markov Model，HMM]]></title>
    <url>%2F2018%2F11%2F18%2FHMM%2F</url>
    <content type="text"><![CDATA[隐马尔可夫模型在词性标注上的应用 给定前提我们只关注N/M/V三个词性 首先介绍第一个概念 Emission Probabilities&gt; 是指同一种状态各个数值得可能性，可以看下图 我们可以看到，当认为是当前词语是N的时候有4/9的可能性是mary接下来是第二个概念 Transition Probabilities&gt; 指状态之间的转换概率，S/E分别表示开始和结束，看下图 OK，那我们先来看一个例子 Jane will spot Will 我们根据前序经验统计出前面的两个概率，然后可以构造流程图如下 我们要计算每一条链的可能性就是把边数值与节点数值一路相乘，为了减少计算，我们采用每一层的动态规划（就是最简单那种），每个节点只保留前序节点数值最大的那一条。最后我们就可以得到这样一条,通过分析我们可以发现确实得到的结果和我们预期相同 以上是找best path，接下来我们看一下HMM的forward algorithm 已知概率分布 1. Initial Sunny Rainy 0.5 0.5 2. Emission Probabilities null yes no sunny 0.1 0.9 rainy 0.8 0.2 3. State transition probabilities null sunny rainy sunny 0.8 0.2 rainy 0.4 0.6 现在给定一个序列 S = [‘yes’, ‘no’, ‘yes’]通过forward algorithm可以计算出在所有的天气组合当中所有满足该序列的概率\$$P(S|1-sunny) = 0.5 * 0.1 = 0.05$$ $$P(S|1-rainy) = 0.5 * 0.8 = 0.4$$ $$P(S|2-sunny) = (P(S|1-sunny)*0.8 + P(S|1-rainy)*0.4)P(no|sunny)$$ 以此类推，最终结果为\(P(S|3-sunny) + P(S|3-rainy)\)我们可以看出来这个过程相当繁琐, 这里用pomegranate库来实现 参考的问题是关于一个海藻的理论推导 https://blog.csdn.net/TH_NUM/article/details/51570174 为什么和手动算的结果有些偏差呢，因为这个每一步都会进行估算1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859from pomegranate import State, HiddenMarkovModel, DiscreteDistributionimport numpy as npmodel = HiddenMarkovModel(name="Example Model")# 设置每个状态的发射概率sunny_emissions = DiscreteDistribution(&#123;"Dry": 0.6, "Dryish": 0.2, "Damp":0.15, "Soggy":0.05&#125;)sunny_state = State(sunny_emissions, name="Sunny")cloud_emissions = DiscreteDistribution(&#123;"Dry": 0.25, "Dryish": 0.25, "Damp":0.25, "Soggy":0.25&#125;)cloud_state = State(cloud_emissions, name="Cloud")rainy_emissions = DiscreteDistribution(&#123;"Dry": 0.05, "Dryish": 0.10, "Damp":0.35, "Soggy":0.50&#125;)rainy_state = State(rainy_emissions, name="Rainy")print(cloud_emissions.probability("Dryish"))# 添加状态model.add_states(sunny_state, cloud_state, rainy_state)# 添加转移概率，从init开始model.add_transition(model.start, sunny_state, 0.63)model.add_transition(model.start, rainy_state, 0.20)model.add_transition(model.start, cloud_state, 0.17)model.add_transition(sunny_state, sunny_state, 0.5) # 50% sunny-&gt;sunnymodel.add_transition(sunny_state, rainy_state, 0.125) # 12.5% sunny-&gt;rainymodel.add_transition(sunny_state, cloud_state, 0.375)model.add_transition(rainy_state, sunny_state, 0.25) # 25% rainy-&gt;sunnymodel.add_transition(rainy_state, rainy_state, 0.375) # 37.5% rainy-&gt;rainymodel.add_transition(rainy_state, cloud_state, 0.375)model.add_transition(cloud_state, cloud_state, 0.125)model.add_transition(cloud_state, sunny_state, 0.25)model.add_transition(cloud_state, rainy_state, 0.625)# 最后使用bake完结model.bake()observations = ['Dry', 'Damp', 'Soggy']forward_matrix = np.exp(model.forward(observations))# TODO: use model.log_probability() to calculate the all-paths likelihood of the# observed sequence and then use np.exp() to convert log-likelihood to likelihoodprobability_percentage = np.exp(model.log_probability(observations))# Display the forward probabilitiesprint(" " + "".join(s.name.center(len(s.name)+6) for s in model.states))for i in range(len(observations) + 1): print(" &lt;start&gt; " if i==0 else observations[i - 1].center(9), end="") print("".join("&#123;:.0f&#125;%".format(100 * forward_matrix[i, j]).center(len(s.name) + 6) for j, s in enumerate(model.states)))print("\nThe likelihood over all possible paths " + \ "of this model producing the sequence &#123;&#125; is &#123;:.2f&#125;%\n\n" .format(observations, 100 * probability_percentage))]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Basic modules of NLP python]]></title>
    <url>%2F2018%2F11%2F18%2Fpython_models%2F</url>
    <content type="text"><![CDATA[指定二维数组, 可以调用a.keys()来查看索引 1a = defaultdict(list) Cunter 可以统计一个list中各个部件出现的数量，可以搭配上面的defaultlist使用 1from collections import Counter]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spam classification --- naive bayes]]></title>
    <url>%2F2018%2F11%2F16%2Fspam-classification%2F</url>
    <content type="text"><![CDATA[举个栗子我们在清分的时候认为有easy和money就可能使垃圾邮件，通过检查邮件中有两个单词就可以说明是否为spam，“朴素”贝叶斯朴素在于认为各个元素是相互独立的，因此直接将概率相乘$$(1) P(spam) P(‘easy’|spam) P(‘money’|spam) + P(ham) P(‘easy’|ham) P(‘money’|ham) = \alpha$$ $$(2) P(spam|’easy’, ‘money’) = \frac{ P(spam) P(‘easy’|spam) P(‘money’|spam) }{\alpha}$$ $$(3) P(ham|’easy’, ‘money’) = \frac{ P(ham) P(‘easy’|ham) P(‘money’|ham) }{\alpha}$$ $$ Final : P(spam|’easy’, ‘money’) + P(ham|’easy’, ‘money’) = 1$$ 当然我们在实际过程中调用sklearn.naive_bayes.MultinomialNB就可以&gt; 注意训练数据的形式是dataframe，用pandas直接读取或者是用matrix转换1234567891011121314151617181920212223242526272829303132333435363738394041424344import pandas as pd# Dataset from - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collectionfrom sklearn.feature_extraction.text import CountVectorizerdf = pd.read_table( './smsspamcollection/SMSSpamCollection', sep='\t', names = ['label', 'sms_message'] )# Note1. 注意读出的是data_frame，命名用namesdf['label'] = df.label.map(&#123;'spam':1, 'ham':0&#125;)# Note2. 二分类问题from sklearn.cross_validation import train_test_splitX_train, X_test, y_train, y_test = train_test_split(df['sms_message'], df['label'], random_state=1)count_vector = CountVectorizer()"""fit是找到规律，如果fit过之后就可以直接transform，因为规律已经学会了"""training_data = count_vector.fit_transform(X_train)testing_data = count_vector.transform(X_test)from sklearn.naive_bayes import MultinomialNBnaive_bayes = MultinomialNB()naive_bayes.fit(training_data, y_train)predictions = naive_bayes.predict(testing_data)"""Precision tells us what proportion of messages we classified as spam, actually were spam.[True Positives/(True Positives + False Positives)]Recall tells us what proportion of messages we classified as spam in the total number of spam[True Positives/(True Positives + False Negatives)]"""from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_scoreprint('Accuracy score: ', format(accuracy_score(y_test, predictions)))print('Precision score: ', format(precision_score(y_test, predictions)))print('Recall score: ', format(recall_score(y_test, predictions)))print('F1 score: ', format(f1_score(y_test, predictions)))]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP(First) Text Processing]]></title>
    <url>%2F2018%2F11%2F14%2FNLP1%2F</url>
    <content type="text"><![CDATA[basic rules of text procession and how to use nltk下图是一个简单的处理流程 判断一段文字中单词出现的数量是一个经典的问题，首先建立一个dict。接着把str用spilt给分开，用正则表达式去除标点，然后统计数量 12345678910111213141516171819202122def count_words(str): """Count how many times each unique word occurs in text.""" counts = dict() # dictionary of &#123; &lt;word&gt;: &lt;count&gt; &#125; pairs to return # text = str(text) # str = "one and two and three and two and one\nbuffalo buffalo buffalo, buffalo buffalo!" str = str.replace("\n", " ") str = str.lower() word_l = str.split(" ") # TODO: Split text into tokens (words), leaving out punctuation # (Hint: Use regex to split on non-alphanumeric characters) # TODO: Aggregate word counts using a dictionary for word in word_l: word = re.match("([a-zA-Z]+).*", word) word = word.group(1) if word not in counts: counts[word] = 1 else: counts[word] += 1 return counts 看起来不错，但是如果有一套统一的工具来做这些是不是会更好，这时候就出现了nltk(Natural Language ToolKit) pip install nltk 1. from nltk.tokenize import word_tokenize 将一个句子中的单词一个个提取出来，相比于自己split好在他更智能。e.g. 可以提取出Dr. 2. from nltk.tokenize import sent_tokenize 可以将一个个句子提取出来 3. from nltk.corpus import stopwords 有一些句子中的单词是没有意义的，stopwords可以帮助我们快速提取出来 4. Sentence Parsing 根据语法规则把一句话变成一棵树，没搞懂啥意思 5. Stemming &amp; Lemmatization 同一个单词可能有不同时态、单复数等，取其枝干可以大大减小运算量和内存占用 另外我们需要了解的还有beautiful的相应用法，imooc的爬虫课程讲解了基础。 正则表达式也是很重要的知识，在这里就不一一阐述]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pyspider (Four) how to use a simple Spider]]></title>
    <url>%2F2018%2F11%2F11%2Fpyspider4%2F</url>
    <content type="text"><![CDATA[一、创建环境使用pycharm安装spyder打开想要创建的目录 12scrapy startproject ArticleSpyder #创建工程scrapy genspider jobbole blog.jobbole.com #创建模板 这时候我们就会发现用pycharm打开这个文件~在setting.py中将obey robots.txt 设置为false防止去多url被过滤在ArticleSpider下创立main.py 12345678from scrapy.cmdline import executeimport sysimport ospath = os.path.dirname(os.path.abspath(__file__))print(path)sys.path.append(path)execute(["scrapy","crawl", "jobblole"]) 二、在cmd中进行实验好处是不用反复run对网页内容进行拉取 1scrapy shell http://blog.jobbole.com/114461/ 三、使用xpath这里可以一次拉取我们需要的信息，接着我们选取拉回的response进行操作这里我们选取一篇可怜的博客作为实验对象，分别拉取他的title、create_date、praise_nums 实验对象 http://blog.jobbole.com/114461/ 这里我们分别采用绝对路径、选取全部class名字、选取部分class名字进行操作。extract()帮助我们提取出里面的有效信息，操作时候注意我们需要的文本是在 当前class下还是h10等小标签下 注意：在python project中在def parse(self, response)下进行操作 1234567891011121314#这里是用绝对路径并extract出里面的文本信息 title = response.xpath("/html/head/title/text()") title.extract() # //p: 所有的p标签 、[@class=xxx]:class 名字为xxx# strip():去掉空格和换行create_date = response.xpath("//p[@class='entry-meta-hide-on-mobile']/text()").extract()[0].strip()create_date = create_date.replace("`","").strip()#用了contains函数：注意要加[]以及'，'分割两个参数praise_nums = response.xpath("//span[contains(@class,'vote-post-up')]/h10/text()").extract() favor = response.xpath("//span[contains(@class,'bookmark-btn')]/text()").extract() 全部操作完成后可以用正则表达式进行清洗 123456789favor = response.xpath("//span[contains(@class,'bookmark-btn')]/text()").extract()[0] match_f = re.match(".*(\d+).*",favor) print("sdfg") if match_f: print(match_f.group(1))# given a tag_list 我们要去除掉不是tag的评论[element for element in tag_list if not element.endswith("评论")] ``` 四、使用css*: 选择所有 #container: 选择id为container的节点.container: 选取所有class包含container的节点li a: 选取所有li下的所有a节点ul + p: 选择ul后面的第一个p元素div#container &gt; ul: 选取id为container的div的第一个ul元素ul ~ p : 选取和ul相邻的所有p元素a[href=”http://jobbole.com&quot;]: 选出所有该gref的所有元素a[href*=”jobblole”]: 选出所有该gref的所有元素a[href^=”http”]: 选出所有该gref以http的所有元素a[href$=”.jpg”]: 选出所有该gref以jpg结尾的所有元素 #id: id 写法 爬取例子 http://blog.jobbole.com/107390/ 选取p元素下的entry-meta-hide-on-mobile类的text 1date = response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip() 取span下vote-post-up中和h10的文本（点赞数），第一个用’.’,后面的用空格，text用冒号。如果说后面的class唯一可以省去span 1vote = response.css("span.vote-post-up h10::text").extract()[0].strip() 取herf=”#article-comment”下的span中的文字 1comment = response.css("a[href='#article-comment'] span::text").extract()[0] 提取内容 1content = response.css(".entry") 提取tag并用’,’连接 12tags = response.css("div.entry-meta p a::text").extract()t = ",".join(tags) 提取一个页面中的所有url 1post_urls = response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() 选取下一页 注意attr的提取功能 1next_p = response.css(".next.page-numbers::attr(href)").extract_first("") 五、实战这里记录下来debug了两个小时的坑123name = "jobblole" allowed_domains = ["web.jobbole.com"] start_urls = ['http://web.jobbole.com/all-posts/'] 一直没注意domain，我们应该确保搜索的范围在domain中，不然会出现错误接着打开一中创建的模板 12345678910111213141516171819# 递归运行函数 def parse(self, response): """ 1. 获取文章列表页中的文章url并交给scrapy下载后并进行解析 2. 获取下一页的url并交给scrapy进行下载， 下载完成后交给parse """ # 解析列表页中的所有文章url并交给scrapy下载后并进行解析 post_nodes = response.css("#archive .floated-thumb .post-thumb a") for post_node in post_nodes: post_url = post_node.css("::attr(href)").extract_first("") print(post_url) yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) # 提取下一页并交给scrapy进行下载 next_url = response.css(".next.page-numbers::attr(href)").extract_first("") if next_url: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse) 这里有三个需要注意的地方 yield Request就是运行，无需其他操作 url要使用parse.urljoin(response.url, post_url), 比如github，能抓取到的只有仓库名，但是前面需要加上github的大域名 callback不需要加括号，只需要函数名字 12345678910111213def parse_detail(self, response): # 通过css选择器提取字段 title = response.css(".entry-header h1::text").extract()[0] create_date = response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·", "").strip() praise_nums = response.css(".vote-post-up h10::text").extract()[0] fav_nums = response.css(".bookmark-btn::text").extract()[0] match_re = re.match(".*?(\d+).*", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 未完待续….等项目全部做完附赠项目地址]]></content>
  </entry>
  <entry>
    <title><![CDATA[Pyspider (Third) Basic knowledge]]></title>
    <url>%2F2018%2F11%2F10%2Fpyspider3%2F</url>
    <content type="text"><![CDATA[URL网络实际是一个树形结构，因此分为广度优先和深度优先搜索。真实网站是存在许多环路的，因此一个重要的方法就是去重。本文介绍去重和字符串编码问题 一、Depth-First &amp; Width-First不做赘述。 二、爬虫去重序列 url经过md5等方法哈希后保存到set中 用bitmap方法，将url hash到某一位（缺点：冲突不命中会比较高） 用bloomfilter对bitmap进行优化 三、Unicode &amp; utf8Unicode将所有语言统一到一套编码，都用2byte表示。但是如果说一篇文章全是英文，储存空间和传输量会比Ascii多一倍utf-8将英文又变回一个字节将UTF-8文件读取成Unicode（方便统一操作），处理完后保存成UFT-8文件（节省空间）python3现在用unicode统一进行表示 12s = "我爱python"s.encode("utf8") # 此时s必须是unicode，不然会报错 前面介绍了基本背景(First)，正则表达式(Second)，和去重及编码，下章节开始讲解Scrapy框架]]></content>
  </entry>
  <entry>
    <title><![CDATA[Pyspider (Second) regular expression]]></title>
    <url>%2F2018%2F11%2F10%2Fpyspider2%2F</url>
    <content type="text"><![CDATA[一、基本规则 ^b: 强制以b开头 .: 可以匹配任意字符 *: 可以代表无限多个前一字符 3$: 必须以3强制结尾 (): 返回括号内匹配的内容 ？: 非贪婪匹配，遇到该字符的第一个就停下 123456789101112import reline = "waaaaangww123"regex_str = ".*?(w.*w).*" # waaaaangww"""regex_str = ".*?(w.*?w).*" # waaaaangwregex_str = ".*(w.*w).*" # ww"""match_str = re.match(regex_str, line)if match_str: print(match_str.group(1)) +: 前面的字符至少出现一次(与都是次数限定符){2}: 前面的出现2次 {2,}:前面的出现至少两次 {2，5}: 前面的出现至少两次至多5次|：或, 模式1或者模式2 (优先提取竖线前的模式)[abcd]:前面的字符是abcd中任意一个均可[0-9]:区间任意一个字符[^1]: 不为1[.]: 去除特殊字符的特殊含义 123456789import re# 举个例子提取电话号码phone_num = "17673168577"regex_str = "(1[5678][0-9]&#123;9&#125;)" # "(1[5678][^1]&#123;9&#125;)"# 17673168577# 以1开头，后面跟5or6or7or8, 再跟9个任意数字match_str = re.match(regex_str, phone_num)if match_str: print(match_str.group(1)) \s: 空格(小写)\S: 单一字符且只要不为空格都可以(大写)\w: 与[A-Za-z0-9_]相同\W: 与上面的相反[\u4E00–u9FA5]: 任意汉字]]></content>
  </entry>
  <entry>
    <title><![CDATA[Introduction of Pyspider(First)]]></title>
    <url>%2F2018%2F11%2F09%2Fpyspider1%2F</url>
    <content type="text"><![CDATA[一、mysql fo navicat连接权限问题 在windows上下载navicat 在linux上配置mysql并在win下用navicat进行连接 mysql配置文件修改 外部访问：/etc/mysql/mysql.conf.d/mysqld.cnf编辑文件：bind-address=0.0.0.0 12GRANT ALL PRIVILEGES ON *.* to 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;flush privileges; #刷新权限 二、用什么技术requests和beautifulsoup都是库，而scrapy是框架。因此本教程运用scrapy Scrapy内置的css和xpath selector方便 三、网页分类 静态网页（例如hexo） 动态网页（例如淘宝） webservice 四、能做什么 搜索引擎—Baidu 推荐引擎—今日头条 机器学习的数据样本 数据分析 ….]]></content>
  </entry>
  <entry>
    <title><![CDATA[Super-Resolution Research]]></title>
    <url>%2F2018%2F11%2F04%2Fsuper-resolution%2F</url>
    <content type="text"><![CDATA[Adapt Super-resolution into real production08/2018-11/2018 Tutor: Bart Selman from Cornell University Purpose: Used symmetric padding to improve perceptual quality of image after super-resolution Duties: Edited code to realize VDSR using tensorflow and symmetric padding to process the images with three channels; Used EC2 of AWS to do experiments using classic Datasets (ImageNet, Set5, Set14) and evaluated the outcomes via PSNR and SSIM; Wrote the Introduction and Experiment&amp;Analysis of the final paper Final paper and Recommendation Letter (&lt;-CLICK)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Reinforment Learning(Five) Actor-Critic]]></title>
    <url>%2F2018%2F11%2F04%2FReinforment_Learning5%2F</url>
    <content type="text"><![CDATA[这个方法比较综合，结合了Deep Q-learning Network(DQN)以及Policy-Based Method，通过搭建两个神经网络实现目标buffer、fixed-Q 一、首先看一下action的职责，基于policy-based。当输入不同state的时候，神经网络可以帮我们自动计算出当前采取各个动作的概率。而训练过程我们只需要\(action \_porb\)和\(TD\_error\)即可。\(TD\_error\)是Critic传过来的，actor部分要做的就是通过神经网络得到当前动作，拿到Critic给的 \(TD\_error\)反向传播 构建网络 def learn： 通过state和当前的action(构建one-hot来选择state得到的结果)，以及critic传入的TD-error得到self.exp_v传给优化器优化 choose_action : 从概率数组里按照动作的概率选择动作 注意： 公式\(TD\_error = (r+gamma*V\_next) - V\_eval\)用来说明当前是否比平均状况好，\(TD\_error\)为正且越大说明当前动作选择的越好 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Actor(object): def __init__(self, sess, n_features, n_actions, lr=0.001): self.sess = sess self.s = tf.placeholder(tf.float32, [1, n_features], "state") self.a = tf.placeholder(tf.int32, None, "act") self.td_error = tf.placeholder(tf.float32, None, "td_error") # TD_error with tf.variable_scope('Actor'): l1 = tf.layers.dense( inputs=self.s, units=20, # number of hidden units activation=tf.nn.relu, kernel_initializer=tf.random_normal_initializer(0., .1), # weights bias_initializer=tf.constant_initializer(0.1), # biases name='l1' ) self.acts_prob = tf.layers.dense( inputs=l1, units=n_actions, # output units activation=tf.nn.softmax, # get action probabilities kernel_initializer=tf.random_normal_initializer(0., .1), # weights bias_initializer=tf.constant_initializer(0.1), # biases name='acts_prob' ) with tf.variable_scope('exp_v'): log_prob = tf.log(self.acts_prob[0, self.a]) self.exp_v = tf.reduce_mean(log_prob * self.td_error) # advantage (TD_error) guided loss with tf.variable_scope('train'): self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v) # minimize(-exp_v) = maximize(exp_v) def learn(self, s, a, td): s = s[np.newaxis, :] feed_dict = &#123;self.s: s, self.a: a, self.td_error: td&#125; _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict) return exp_v def choose_action(self, s): s = s[np.newaxis, :] probs = self.sess.run(self.acts_prob, &#123;self.s: s&#125;) # get probabilities for all actions return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel()) # return a int 二、看一下Critic的职责。拿到了\(state，next\_state,reward\), 计算得到TD_error, 进行优化器优化，并传给actor 构建网络，state对应唯一输出，意思就是当前state应该对应的\(value\_eval\)（平均值，期望），更加简单 \(loss={TD\_error}^{2}\), 并不断减小这个loss。 def learn：传入当前\(s, reward, s\_next\)运行tf得到error 12345678910111213141516171819202122232425262728293031323334353637383940414243class Critic(object): def __init__(self, sess, n_features, lr=0.01): self.sess = sess self.s = tf.placeholder(tf.float32, [1, n_features], "state") self.v_ = tf.placeholder(tf.float32, [1, 1], "v_next") self.r = tf.placeholder(tf.float32, None, 'r') with tf.variable_scope('Critic'): l1 = tf.layers.dense( inputs=self.s, units=20, # number of hidden units activation=tf.nn.relu, # None # have to be linear to make sure the convergence of actor. # But linear approximator seems hardly learns the correct Q. kernel_initializer=tf.random_normal_initializer(0., .1), # weights bias_initializer=tf.constant_initializer(0.1), # biases name='l1' ) self.v = tf.layers.dense( inputs=l1, units=1, # output units activation=None, kernel_initializer=tf.random_normal_initializer(0., .1), # weights bias_initializer=tf.constant_initializer(0.1), # biases name='V' ) with tf.variable_scope('squared_TD_error'): self.td_error = self.r + GAMMA * self.v_ - self.v self.loss = tf.square(self.td_error) # TD_error = (r+gamma*V_next) - V_eval with tf.variable_scope('train'): self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss) def learn(self, s, r, s_): s, s_ = s[np.newaxis, :], s_[np.newaxis, :] v_ = self.sess.run(self.v, &#123;self.s: s_&#125;) td_error, _ = self.sess.run([self.td_error, self.train_op], &#123;self.s: s, self.v_: v_, self.r: r&#125;) return td_error 三、两者实现交互1234567a = actor.choose_action(s)s_, r, done, info = env.step(a) td_error = critic.learn(s, r, s_) # gradient = grad[r + gamma * V(s_) - V(s)] actor.learn(s, a, td_error) # true_gradient = grad[logPi(s,a) * td_error] Refer : https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/6-1-actor-critic/]]></content>
  </entry>
  <entry>
    <title><![CDATA[Reinforment Learning(Four) Policy Based Method]]></title>
    <url>%2F2018%2F11%2F04%2FReinforcement_Learning4%2F</url>
    <content type="text"><![CDATA[Policy Based method莫烦python Policy Based Method意思是给特定的状态特定的动作 ,适用于一个连续的运动 , 相比于Value-Based方法，我们只在乎当前状态经过神经网络运算出来的action。通过Reward进行训练神经网络的参数\(w\)，而不考虑其数值的算法进行收敛。随机性搜索策略 (这里以蒙特卡洛为例子，即有限个动作)$$\theta = \theta + \alpha \bigtriangledown_{\theta}log \pi_{\theta}(s_{\tau},a_{t}, \theta)v_{\tau}，可以看到v_{\tau}越大，更新的幅度越大$$ $$R_{\tau} = R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ….$$ 1234neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)##这里log有符号，因此minimize其实是增大loss = tf.reduce_mean(neg_log_prob * self.tf_vt) # 对于不同action，他们的tf.vt不同，train的过程都是增加，但是vt越大增加的概率越大，训练到最后会选择那些vt大的动作 \(v_{t}\)是指奖惩的大小，如果我们发现奖惩很好，那么调整神经网络使得这个动作以后多被选中一些。 如果觉得不好，则下一次选中的机会会变小 但是对于没有终点的任务，比如我们训练一个小孩子上学、吃饭、睡觉、再上学….显然这个是没有终点的，我们找不到一个合适的时间点来更新我们的\(w\)，因此我们采用下面的公式 $$\Delta \theta = \alpha \ \bigtriangledown_{\theta}(log (S_{t}, A_{t}, \theta) Q(S_{t},A_{t}), 我们采用Q(S_{t},A_{t}来替换R_{\tau}$$ $$Q(S_{t},A_{t} )= Q(S_{t}, A_{t}) + \beta(R_{t+1} + \gamma Q(S_{t+1}, A_{t+1} - Q(S_{t},A_{t}))$$ actor-critic method用两套神经网络，一套用于预测动作，另一套用于评估动作的好坏。 \(\theta \)是Policy Based 来选择动作，\(w\)是前面DQN来生成反馈\(\hat{q}(S, A)\)。因此这个方法也叫做Actor-Critic actor来源于policy-based， critic来源于value-based。actor来指手画脚，critic告诉他哪一个动作是对，哪一个是错 $$Actor : \Delta \theta = \alpha \bigtriangledown_{\theta}(log\pi(S_{t},A_{t},\theta))\hat{q}(S_{t}, A_{t}, w)\pi 是选取动作的指令，q是反馈的动作值$$ $$Critic : \Delta w = \beta(R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t}, w)) - \hat{q}(S_{t}, A_{t}, w)) \bigtriangledown_{w} \hat{q}(S_{t}, A_{t}, w)$$ Deep Deterministic Policy Gradient(DDPG)Deep: 仿照DQN，有一个buffer，两个更新频率不相同的神经网络参数\(w\) , 由deepMind对actor-critic method]]></content>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning（First）]]></title>
    <url>%2F2018%2F11%2F02%2FReinforce_Learning1%2F</url>
    <content type="text"><![CDATA[我看到网上的强化学习教程通常比较复杂, 看完莫烦python后总结出来如下, 我将主要用代码形式进行理论的展现, 其中个别地方用的是OPENAI的表示方式 一、回合更新(Monte-Carlo update)玩完所有的步数, 等到一个episode结束之后再更新数值 Monte-Carlo Learning这个方法比较esay, 就是根据尽可能多的经验, 以及平均期望来更新数值。实验成本大, 并且现在很多需要决策的问题是没有结束标志的。 二、单步更新(Temporal-Difference update)每走一步都会更新当前的数值 Q-Learning(off-policy), Sarsa(on-policy) 1. 什么是off/on-policy?off-policy的更新value时候的next_action不一定会真实采取, 而on-policy更新时候的next_value就是真实采取的 小明正在准备高考, 课间的时候小明在思考接下来学习什么知识。首先, 小明想学习数学, 但是小明的数学已经学习的很熟练了，于是小明并没有拿出课本, 而是在脑海里把课本背诵了一边, 这样一来, 小明虽然没有拿出课本, 但是依然更新了当前的知识, 接下来小明可能拿出的是英语/语文/物理课本进行学习。这就是我们的off-policy。倘若是on-policy, 小明就失去了默背这一流程, 想回忆一下数学下一步骤必须拿出数学课本 2. epsilon_greedy策略 取到最大值的概率为 (1-eps) + (eps)/n取到其他n-1种动作的概率为 (eps)/n 12345678910111213141516def epsilon_greedy(Q, state, nA, eps): """Selects epsilon-greedy action for supplied state. Params ====== Q (dictionary): action-value function state (int): current state nA (int): number actions in the environment eps (float): epsilon """ if random.random() &gt; eps: # 此时是选取最大数值的action进行返回 return np.argmax(Q[state]) else: # 这个时候对每个取平均 return random.choice(np.arange(env.action_space.n)) 3. Q-learning with sarsa_max?(off-policy)sarsa_max: 选取下一状态是value最大的动作更新当前状态 1234567891011# Q-learning Sarsamaxdef update_Q_sarsamax(alpha, gamma, Q, state, action, reward, next_state=None): # Returns updated Q-value for the most recent experience. current = Q[state][action] # estimate in Q-table (for current state, action pair) Qsa_next = np.max(Q[next_state]) if next_state is not None else 0 # find the max q in next_state, but this action is not the real action target = reward + (gamma * Qsa_next) # construct TD target new_value = current + (alpha * (target - current)) # get updated value return new_value 1234567891011121314# Q-learning while True: action = epsilon_greedy(Q, state, nA, eps) # epsilon-greedy action selection, get next_action in every interation next_state, reward, done, info = env.step(action) # take action A, observe R, S' score += reward # add reward to agent's score Q[state][action] = update_Q_sarsamax(alpha, gamma, Q, \ state, action, reward, next_state) state = next_state #只更新状态 if done: tmp_scores.append(score) # append score break 4. Sarsa Learning每次迭代都会选出下一状态和下一状态将要采取的动作，根据事实(update_Q_sarsa)来进行更新12345678910def update_Q_sarsa(alpha, gamma, Q, state, action, reward, next_state=None, next_action=None): #Returns updated Q-value for the most recent experience. current = Q[state][action] # estimate in Q-table (for current state, action pair) # get value of state, action pair at next time step Qsa_next = Q[next_state][next_action] if next_state is not None else 0 target = reward + (gamma * Qsa_next) # construct TD target new_value = current + (alpha * (target - current)) # get updated value return new_value 123456789101112131415161718while True: next_state, reward, done, info = env.step(action) # take action A, observe R, S' score += reward # add reward to agent's score if not done: next_action = epsilon_greedy(Q, next_state, nA, eps) # epsilon-greedy action Q[state][action] = update_Q_sarsa(alpha, gamma, Q, \ state, action, reward, next_state, next_action) state = next_state # S &lt;- S' action = next_action # A &lt;- A' ''' 这里会根据传入update函数的next_action 和 next_state进行更新 ''' if done: Q[state][action] = update_Q_sarsa(alpha, gamma, Q, \ state, action, reward) tmp_scores.append(score) # append score break 5. expected Sarsa3,4 种描述的更新方法都是取单一action的value, 在期望Sarsa中将对所有的action产生的value计算一个平均期望进行更新 1234567891011def update_Q_expsarsa(alpha, gamma, nA, eps, Q, state, action, reward, next_state=None): """Returns updated Q-value for the most recent experience.""" current = Q[state][action] # estimate in Q-table (for current state, action pair) policy_s = np.ones(nA) * eps / nA # 建立一个向量储存概率，每一个都是 （eps / nA）, nA表示action数量 policy_s[np.argmax(Q[next_state])] = 1 - eps + (eps / nA) # 将最大value的action的概率变为1 - eps + (eps / nA) Qsa_next = np.dot(Q[next_state], policy_s) # get value of state at next time step target = reward + (gamma * Qsa_next) # construct target new_value = current + (alpha * (target - current)) # get updated value return new_value]]></content>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning（Third）Code of DQN with TF]]></title>
    <url>%2F2018%2F11%2F02%2FReinforcement_Learning3%2F</url>
    <content type="text"><![CDATA[算法思想 缓冲区及神经网络先用\(\epsilon greedy\)策略选出下一个action，并得到reward和next_state。将 \(&lt;s_{t}, action, reward, s_{t+1}&gt;\)存入缓冲区1234567891011121314from collections import dequeclass Memory(): def __init__(self, max_size=1000): self.buffer = deque(maxlen=max_size) def add(self, experience): self.buffer.append(experience) ## 添加 def sample(self, batch_size): idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False) return [self.buffer[ii] for ii in idx] ##随机返回一些batch进行学习 构建一个基础的神经网络，将state的单热点编码作为输入，得到若干个action及他们的数值。123456789101112131415161718192021222324252627282930313233import tensorflow as tfclass QNetwork: def __init__(self, learning_rate=0.01, state_size=4, action_size=2, hidden_size=10, name='QNetwork'): # state inputs to the Q-network with tf.variable_scope(name): self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs') # 由于我们都是选定一个动作之后比较Q值，因此在计算时我们会传入当前state选择的action self.actions_ = tf.placeholder(tf.int32, [None], name='actions') one_hot_actions = tf.one_hot(self.actions_, action_size) # targetQ另作计算,在training的时候直接传入 self.targetQs_ = tf.placeholder(tf.float32, [None], name='target') # 构建relu网络 self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size) self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size) # 这里是神经网络的输出，targetQ的计算就是单独得到self.output进行处理 self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, activation_fn=None) ### Train with loss (targetQ - Q)^2 # # 根据state选择的action得到相应的结果，axis=1，说明是行相乘 self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1) # self.targetQs是用神经网络得到不同action的数值后选取max，因此每一个targetQ也是单一数值，与上面的Q对应 self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q)) self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss) \(s_{t}\)与对应的action用来得到\(\hat{Q}(s,a,w)\) then 得到 \(Q(s, max_a, w)\) 算法主体1. 使用\(\epsilon \)策略拿到每一次的action12345678if explore_p &gt; np.random.rand(): # Make a random action action = env.action_space.sample()else: # Get action from Q-network feed = &#123;mainQN.inputs_: state.reshape((1, *state.shape))&#125; Qs = sess.run(mainQN.output, feed_dict=feed) action = np.argmax(Qs) 2. 接着拿到reward、next_state 并存入123next_state, reward, done, _ = env.step(action) #拿到信息memory.add((state, action, reward, next_state)) #存入 3. 得到\reward + (Q(s,a, w)\), 策略是选取神经网络输出的若干个动作value的最大值123456789101112 # Train network target_Qs = sess.run(mainQN.output, feed_dict=&#123;mainQN.inputs_: next_states&#125;) # Set target_Qs to 0 for states where episode ends episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1) target_Qs[episode_ends] = (0, 0) targets = rewards + gamma * np.max(target_Qs, axis=1)``` &gt; ##### 4. 训练 loss, _ = sess.run([mainQN.loss, mainQN.opt], feed_dict={mainQN.inputs_: states, ##当前状态 mainQN.targetQs_: targets, ##已经计算好 mainQN.actions_: actions}) ##当前状态所要选取的动作 ` 点击这里查看完整ipynb]]></content>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning (Second)]]></title>
    <url>%2F2018%2F11%2F01%2FReinforce_Learning2%2F</url>
    <content type="text"><![CDATA[Reference : An introduction to Deep Q-Learning: let’s play DoomDeep Q-Learning Network (DQN)1. 深度强化学习对于一个复杂的游戏，表示动作及其Value、在Q表里搜索相应的动作变得十分没有效率。因此这里改用神经网络进行Q表的计算。 每次将S放入，即可计算出相应的action及对应的value \(w\)表示神经网络的权值，\(R\)表示Reward，\(Q\)表示Q表中相应状态和动作对应的数值 $$current\_predict\_Q = \hat{Q}(s,a,w)$$ $$Grdient = \bigtriangledown \hat{Q}(s,a,w)$$ $$\Delta w(TDerror) = \alpha[R + \gamma max_{a} \hat{Q}(s\prime ,a, w)) - current\_predict\_Q]$$ 2. 主要优化方法 经验回放(Experience Deplay)和固定Q目标是其中的两个主要贡献 经验回放有些动作的代价很大，我们可以把经历过的 \(&lt;S_{t}, A_{t}, R_{t+1}, S_{t+1}&gt;\) 储存在缓冲区中(replay buffer)，后面可以再次用来学习。并可以采取优先经验回收我们认为，loss越大的数值越具有学习价值，因此buffer里的所有数据都根据其loss决定被选择的概率，每一次学习之后都会更新其概率\(p(i)=\frac{p_{i}^{a}}{\sum_{k=1}^{n} p_{k}^{a}}\),这里的\(a\)保证了不完全按照概率，减少过拟合。（a=1时完全按照概率选取） 研究证明，优先经验回收策略可以减少迭代次数 Q固定我们可以看到在我们的\(R + \gamma max_{a} \hat{Q}(s\prime ,a, w)\)和\(\hat{Q}(s,a,w)\)中都有\(w\)存在。因此两者都在变，导致无法持续收敛。 打个比方。小明在追他养的牛，想要不断向他的牛靠近。然而他的牛的位置（Q）因为\(w\)的改变也在不断变化。可能一会在小明前，一会去小明后。小明也会懵逼，到底该往哪个方向追。因此我们将\(w^{-}\)固定，事后更新\(w^{-} \gets w\)。这样保证了在收敛过程中目标是确定的不会变化。 $$\Delta w(TDerror) = \alpha[R + \gamma max_{a} \hat{Q}(s\prime ,a, w^{-})) - \hat{Q}(s,a,w)]\bigtriangledown \hat{Q}(s,a,w)$$ Thrun 和 Schwartz，1993 年，《使用函数逼近进行强化学习存在的问题》（ 高估 Q 值）van Hasselt et al.，2015 年，《双 Q 学习的深度强化学习》Schaul et al.，2016 年，《优先经验回放》Wang et al.，2015 年，《深度强化学习的对抗网络架构》Hausknecht 和 Stone，2015 年，《部分可观察 MDP 的深度递归 Q 学习》]]></content>
  </entry>
  <entry>
    <title><![CDATA[Basic Rules of Tensorflow]]></title>
    <url>%2F2018%2F11%2F01%2Ftensorflow_basic%2F</url>
    <content type="text"><![CDATA[常见符号1. tf.Variable 与 tf.constant 第一个可以根据计算改变，第二个是不能变的, 因此神经网络的权值通常都是Variable另外要注意, 定义了变量和常量之后，要初始化才能使用 123init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) 2. tf.reduce_mean 取平均值，后面跟1是取行，跟0是取列 123456789101112import numpy as npimport tensorflow as tfx = np.array([[1.,2.,3.],[4.,5.,6.]])with tf.Session() as sess: mean_none = sess.run(tf.reduce_mean(x)) mean_1 = sess.run(tf.reduce_mean(x,1)) mean_2 = sess.run(tf.reduce_mean(x,0)) print(x) print(mean_none) print(mean_1) print(mean_2) 3. tf.equal 分别比较矩阵中相同的元素，相同就返回true,不同就返回false 12345678import tensorflow as tf import numpy as np A = [[1,3,4,5,6]] B = [[1,3,4,3,2]] with tf.Session() as sess: print(sess.run(tf.equal(A, B))) 4. tf.truncated_normal shape 表示要生成的矩阵的大小mean 表示正态函数的均值stddev表示要生成的随机数的方差seed 是随机数种，通常条件下为None 1tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 5. tf.reshape 更改矩阵的大小, 通常对图像进行改变参数中可以指定一个 -1 （且只能有一个）, 将根据其他参数的指定自动计算出该维度 1https://blog.csdn.net/m0_37592397/article/details/78695318]]></content>
  </entry>
  <entry>
    <title><![CDATA[一些在图像处理上常用的方法]]></title>
    <url>%2F2018%2F11%2F01%2Fbasic%20operations%20of%20CV%2F</url>
    <content type="text"><![CDATA[1. 获取图像目录 glob, cv2 123456789import globfiles_path = glob.glob(r'flower/*')import cv2img = cv2.imread(files_path[1])img = cv2.reshape(img, (224,224))size = img.size()print(size) 2. 将图像矩阵连接12345codes = Noneif codes is None: codes = code_batchelse: codes = np.concatnate((codes,code_batch)) 3. 将图像矩阵（经过CNN处理完）写入txt，并将结果一并储存123456with open('codes','w') as f: codes.tofile(f) with open('labels','w') as f: writer = csv.writer(f,delimiter='\n') writer.writerow(labels) 4. 将图像矩阵（经过CNN处理完）从txt和CSV读出12345678import csvwith open('labels') as f: reader = csv.reader(f,delimiter='\n') labels = np.array([each for each in reader if len(each) &gt; 0] ).squeeze() with open('codes') as f: codes = np.fromfile(f, dtype=np.float32) codes = np.codes.reshape(len(labels),-1) 5. 图像读出并存回123456789import cv2 as cv# load img = cv.imread(imagepath)# shape=(height, width, channel)h,w,c = img.shape# showcv.imshow('window_title', img)# savecv.imwrite(savepath, img) 6.图像旋转12345678910111213141516171819import cv2from math import *import numpy as npimg = cv2.imread(path[0])height,width=img.shape[:2]degree=45#旋转后的尺寸heightNew=int(width*fabs(sin(radians(degree)))+height*fabs(cos(radians(degree))))widthNew=int(height*fabs(sin(radians(degree)))+width*fabs(cos(radians(degree))))matRotation=cv2.getRotationMatrix2D((width/2,height/2),degree,1)matRotation[0,2] +=(widthNew-width)/2 #重点在这步，目前不懂为什么加这步matRotation[1,2] +=(heightNew-height)/2 #重点在这步imgRotation=cv2.warpAffine(img,matRotation,(widthNew,heightNew),borderValue=(255,255,255)) 7. 图像色道分离与合并# 加载图像 image = cv2.imread(args[&quot;image&quot;]) # 通道分离，注意顺序BGR不是RGB,并且会发现得到的都是灰度图 (B, G, R) = cv2.split(image) # 生成一个值为0的单通道数组 zeros = np.zeros(image.shape[:2], dtype = &quot;uint8&quot;) # 分别扩展B、G、R成为三通道。另外两个通道用上面的值为0的数组填充 cv2.imshow(&quot;Blue&quot;, cv2.merge([B, zeros, zeros])) cv2.imshow(&quot;Green&quot;, cv2.merge([zeros, G, zeros])) cv2.imshow(&quot;Red&quot;, cv2.merge([zeros, zeros, R]))]]></content>
  </entry>
  <entry>
    <title><![CDATA[c++ basic rules]]></title>
    <url>%2F2018%2F11%2F01%2Fc%2B%2B_rules%2F</url>
    <content type="text"><![CDATA[C++ STL1. mapThis can be use there are some simple rules, for example, an integer refers to a string, like in leetcode12 and leetcode1312345678910111213141516//#include &lt;iostream&gt;#include &lt;map&gt;using namespace std; int main()&#123; map&lt;int, string&gt; mymap; mymap.insert(pair&lt;int, string&gt;(3,"sdf"));//using "insert" to do the insert operation map&lt;int, string&gt;::iterator iter;// define an interator for (iter = mymap.begin(); iter != mymap.end(); iter ++)&#123;// from begin() to the end() cout&lt;&lt;iter-&gt;first&lt;&lt;" "&lt;&lt;iter-&gt;second;// first and second value &#125; return 0;&#125; c++ String12int first = str.find(s); // return the first index of substring "s" in "str"int last = str.find(s); // return the last index of substring "s" in "str"]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flower Classification via VGG]]></title>
    <url>%2F2018%2F11%2F01%2Fflower_classification%2F</url>
    <content type="text"><![CDATA[利用vgg迁移学习实现花朵的分类，基于tensorflow进行实现为什么要迁移学习 一个好的CNN可以判断出一个图片的基本轮廓, 我们利用这个训练好的CNN得到轮廓并扁平化处理, 我们要做的就是搭建ANN, 来实现我们自己的classifier一个理解的网站 首先介绍两个比较常用的方法1. 单热点编码 在这里我们有五种花, 他们的名字都是中文, 我们需要对他进行编码 1234567/* labels是我们要编码的str数组，生成了labels_vecs的二维单热点编码 */from sklearn.preprocessing import LabelBinarizerlb = LabelBinarizer()lb.fit(labels)labels_vecs = lb.transform(labels) array([[0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], …, [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0]])12 2. 分类—我们需要随机的分出test、valid和Train 这里使用机器学习库 how to use 123456789101112131415from sklearn.model_selection import StratifiedShuffleSplit/* 固定搭配，里面的参数要弄懂 */ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)/* 注意一定要加next，这里返回的是indices（指数） 即数标 */train_idx, val_idxs = next(ss.split(codes,labels_vecs))half_val = int(len(val_idxs)/2)/* 把valid拆分成valid和test */val_idx,test_idx = val_idxs[:half_val],val_idxs[half_val:]train_x, train_y = codes[train_idx],labels_vecs[train_idx]val_x, val_y = codes[val_idx],labels_vecs[val_idx]test_x, test_y = codes[test_idx],labels_vecs[test_idx] 3. 储存结果 每次训练结束都要把参数的结果储存到checkpoint中来, 下一次test时候直接进行调用 4. 计算主体 外部一个epochs的循环，内部一个计算函数 储存结果12/* 详见training部分 */saver.save(sess, "checkpoints/flowers.ckpt") 加载结果进行使用1234567891011121314/* 先迁移得到一维矩阵，然后Saver加载参数，test一边 */with tf.Session() as sess: img = utils.load_image(test_img_path) img = img.reshape((1, 224, 224, 3)) feed_dict = &#123;input_: img&#125; code = sess.run(vgg.relu6, feed_dict=feed_dict) saver = tf.train.Saver()with tf.Session() as sess: saver.restore(sess, tf.train.latest_checkpoint('checkpoints')) feed = &#123;inputs_: code&#125; prediction = sess.run(predicted, feed_dict=feed).squeeze() data cutting1234567891011121314/* 设置函数10个图片的处理 */def get_batches(x, y, n_batches=10): """ Return a generator that yields batches from arrays x and y. """ batch_size = len(x)//n_batches for ii in range(0, n_batches*batch_size, batch_size): # If we're not on the last batch, grab data with size batch_size if ii != (n_batches-1)*batch_size: X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] # On the last batch, grab the rest of the data else: X, Y = x[ii:], y[ii:] # I love generators yield X, Y building NN123456789101112131415161718inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]])# TODO: Classifier layers and operations/* 搭建一个简单的ANN */fc = tf.contrib.layers.fully_connected(inputs_,256)logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None)cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)cost = tf.reduce_mean(cross_entropy)optimizer = tf.train.AdamOptimizer().minimize(cost)# Operations for validation/test accuracypredicted = tf.nn.softmax(logits)correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) Training123456789101112131415161718192021222324epochs = 10iteration = 0saver = tf.train.Saver()with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for e in range(epochs): for x, y in get_batches(train_x, train_y): feed = &#123;inputs_: x, labels_: y&#125; loss, _ = sess.run([cost, optimizer], feed_dict=feed) print("Epoch: &#123;&#125;/&#123;&#125;".format(e+1, epochs), "Iteration: &#123;&#125;".format(iteration), "Training loss: &#123;:.5f&#125;".format(loss)) iteration += 1 if iteration % 5 == 0: feed = &#123;inputs_: val_x, labels_: val_y&#125; val_acc = sess.run(accuracy, feed_dict=feed) print("Epoch: &#123;&#125;/&#123;&#125;".format(e, epochs), "Iteration: &#123;&#125;".format(iteration), "Validation Acc: &#123;:.4f&#125;".format(val_acc)) saver.save(sess, "checkpoints/flowers.ckpt")]]></content>
  </entry>
  <entry>
    <title><![CDATA[How to apply hexo to Github Page (一)]]></title>
    <url>%2F2018%2F11%2F01%2Fhexo%2F</url>
    <content type="text"><![CDATA[How to build up your hexo on Github PageFirstly, you need to sign up a github account and build a repo which has the same name with your github. github name : JasonWang0808gitpage name : JasonWang0808.github.io Secondly, prepare the hexo The set up steps are as followed:Step1. Install Node.js and Git and make sure they have been installed properly. 1234Win+R cmdnode -vgit -version step2. Prepare the SSH key and goto the github “SSH and GPG keys” to fill in 123456#open the git bashssh-keygen -t rsa# type enter three timescd C:\Users\Administrator\.sshcat id_rsa.pub# copy the content 123#evaluatessh -T git@github.com## return Hi step3. Install hexo 123456789cd xxxnpm install hexo-cli -ghexo init Hexo cd /Hexo npm instal hexo cleanhexo generate（可简写为hexo g） hexo sever（可简写为hexo s）npm install –save hexo-deployer-git step4. Modify the config 12345# use sublime to open /blog/_config.yml and find "deploy"deploy:type: gitrepo: git@github.com:yourname/yourname.github.io.gitbranch: master step5. modify ./config.yml and ./themes/next/config.yml depends on your hobby 1hexo d Step6. next, you can change your theme and add more decorations to your website A good way to learn to decorationstep5. modify ./config.yml and ./themes/next/config.yml depends on your hobby 1hexo d Step7. start to write on your github page with hexo 1234hexo new "xxx"# this can help you to build a new xx.mdhexo new page "xxx"# this help you to build a new page, for example, "categories" Every xx.md has its head, for example, Title is the name. Tags are tags, you know. categories is a big package containing all the essays with this category12345---title: Notes of LeetCode(一)tags: [leetcode, c++,algorithm]categories: leetcodeNotes--- Finally, you can use Tengxun cloud to get a domain for your pageThere are two records you need to add12#get the ip address of your page ping www.xx.github.io]]></content>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming---Leetcode]]></title>
    <url>%2F2018%2F11%2F01%2FDP%2F</url>
    <content type="text"><![CDATA[Dynamic ProgrammingThe situations are that our current situations depend on the old calculations. So we don’t need to start from the begining again, we can store the outcome of each step so we can cite it directly. Usually, the parameters of the function are the index instead of the object (string etc.) itself. Because our operations are on the matrix. Example1. 5. Longest Palindromic Substring from LeetcodeWe calculation according to the length of the string. If we wanna judge ‘cbbaac’, we can find whether ‘bbaa’ is Palindromic, then find the character from the two ends are same. Example2. 10. Regular Expression Matching from the LeetcodeJust like we talk in “Recursion”, DP algorithm sometimes help Recursion to decrease the calculation. Such as: 12345def dp(i, j, arr): if arr[i][j] != null: ## it can returns directly return arr[i][j] else: calculate the arr[i][j]]]></content>
  </entry>
  <entry>
    <title><![CDATA[Notes of LeetCode (a)]]></title>
    <url>%2F2018%2F11%2F01%2Fleetcode%2F</url>
    <content type="text"><![CDATA[3. Longest Substring Without Repeating CharactersⅠ. DATA STRUCTURE: HashMap HashMap is a quick way to find elements in the time complexity of O(1) and is commonly used in leetcode. 1HashMap&lt;Charactor,Integer&gt; map = new HashMap&lt;&gt; (); Ⅱ. Method:Sliding WindowThe sliding window is an abstract concept commonly used in array/String problems. It likes Dynamic Programming, because we don’t know the answer until we finish it. The length is j - i because the arrange is [i, j) (left-closed, right-open). We set the ANS to record the longest answer. Once we find our next number is replucated in our map, we change the i to the number in our map, same with truncating. Then next, until we reach the end of the array. 1234567891011121314151617class Solution &#123; //map.put map.get map.containsKey public int lengthOfLongestSubstring(String s) &#123; HashMap&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); int i = 0, j = 0, ans = 0; int l = s.length(); while(i &lt; l &amp;&amp; j &lt; l)&#123; if(map.containsKey(s.charAt(j)))&#123; //if next element in the map i = Math.max(map.get(s.charAt(j)) + 1, i); // abba, max is to prevent : "a" in the end makes i be '0' &#125; ans = Math.max(ans, j-i+1); // keep the bigger ans map.put(s.charAt(j), j); // refresh the map j ++; // goto next number &#125; return ans; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[OBject claasification via Resnet50]]></title>
    <url>%2F2018%2F11%2F01%2FModel_detexting_1%2F</url>
    <content type="text"><![CDATA[how to detect a model1234567891011121314from keras.preprocessing import image from tqdm import tqdmdef path_to_tensor(img_path): # loads RGB image as PIL.Image.Image type img = image.load_img(img_path, target_size=(224, 224)) # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3) x = image.img_to_array(img) # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor return np.expand_dims(x, axis=0)def paths_to_tensor(img_paths): list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)] return np.vstack(list_of_tensors) you can check in this Dictionary 123456from keras.applications.resnet50 import preprocess_input, decode_predictionsdef ResNet50_predict_labels(img_path): # returns prediction vector for image located at img_path img = preprocess_input(path_to_tensor(img_path)) return np.argmax(ResNet50_model.predict(img))]]></content>
  </entry>
  <entry>
    <title><![CDATA[Recursion---leetcode]]></title>
    <url>%2F2018%2F11%2F01%2FRecursion%2F</url>
    <content type="text"><![CDATA[RecursionFrom my perspective, Recursion is an efficient way because it makes us to only look at the current step. However, its complexity is very high. Example1. 10. Regular Expression Matching from Leetcode You can see the description on the above link. My problem is that it contains tooooo many different combination of “character”, “.” and “*“. It looks like a time array, which you should look at the moment. time before and the future, which is very bothering. So what we do here is only determine the current action, and let the function to judge what to do next itself. So, given two arries, we have several choices whether reach the end?12345678910# p and s reach the end at the same time# p reach the end, but s doesn'tdef isMatch(s, p)&#123; if(p.empty())&#123; if(s == empty()) return 1; else return 0 &#125;&#125;``` 2. if the first character match? def isMatch(s, p){ bool first_match; if(s[0] == p[0] || p[0] == ‘.’) first_match = true; }13. is it followed by * ? def isMatch(s, p){ if(p.length() &gt;= 2 || p[1] == ‘‘): # the second character of p is # this if means: * is 0 or not return isMatch(s, p.substr(2)) || (first_match &amp;&amp; isMatch(s.substr(1), p) else: return first_match &amp;&amp; isMatch(s.substr(1), p.substr(1)) }1The complete code in C++ #include using namespace std;class Solution {public: bool isMatch(string s, string p) { bool first = false; if(p.empty()){ return (s.empty()); } else{ first = (!s.empty() &amp;&amp; (s[0] == p[0] || p[0] == ‘.’)); } if(p.length()&gt;= 2 &amp;&amp; p[1] == ‘‘){ return (isMatch(s, p.substr(2)) || (first &amp;&amp; isMatch(s.substr(1), p) ) ); // make be 0 || make * be at least 1 } else return first &amp;&amp; isMatch(s.substr(1), p.substr(1)); }};int main() { Solution solution; return 0; }`We can see that the compexity is high. Because there are many duplicated calculations. SO we can set an array to solve this problem, this is same with DYNAMIC PROGRAMMING, which we will dicuss in there.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Welcome]]></title>
    <url>%2F2018%2F10%2F20%2FNew%20begining%2F</url>
    <content type="text"><![CDATA[Welcome to JunYu Wang’s siteI am proud that I’ve keep writing blogs for almost half year in 2018. The content are as followed. Notes of the CS system and data structure from the class Notes of the Deep-learning courses from Udacity …. The reason why today is very important is that I realize that my blogs shouldn’t have been regarded as merely “records of my study”. It should be more formal, more beautiful, more ordered, and most importantly, more inspiring.I always like new beginings and new challanges, which make me feel alive. So I want to delve furture into some specific areas of study to equip myself to be more professional. Notes of how to deal with the problems on Leetcode Notes of deeper details of algorithm such as object detection Notes of new classes in new school ….. CLICK “About” to see my RESUME CLICK “Categories” to see all the topics of my articles CLICK “Tags”… OOPS, this is under building CLICK “Search” to find the topic you have interests on Hope you Enjoy this 1234567891011if (you have any questions)&#123; case email: wangjunyu0808@gmail.com; case insgram: wangjunyu0808; case wechat: HeNeArKrXnTn default;&#125;I will be happy to share ideas with you]]></content>
  </entry>
</search>
